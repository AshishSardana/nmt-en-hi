{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "With the introduction of the [Samanantar dataset](https://indicnlp.ai4bharat.org/samanantar/), Ramesh et al. have released a parallel corpus for 11 Indic languages with over 49.7 million sentence pairs which can be used for training Indic NMT models. \n",
    "\n",
    "To facilitate English to Hindi MT, we will train the NMT model on the en-hi subset of the Samanantar dataset using NVIDIA NeMo. NVIDIA NeMo is an open-source conversational AI toolkit that allows developers to build and train state-of-the-art models.\n",
    "\n",
    "The NMT model which we would train is based on the transformer architecture. It is a powerful seq2seq modeling architecture as described by Vaswani et al. in the paper Attention Is All You Need. \n",
    "\n",
    "Samanantar is the largest available parallel corpora supporting 11 Indic languages containing over 49.6M sentence pairs. The data includes samples from previously available datasets and samples mined from the web. In this blog, we will be using the En-Hi pair containing over 8.46M samples. For our experiments, we will use a 90:10 train-val split to train our models, which will then be evaluated on standard test sets. The dataset contains sentences with word counts ranging from one to thousands.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path where we will download the dataset\n",
    "dataset_path = '../dataset/'\n",
    "%env dataset_path = {dataset_path}\n",
    "!mkdir -p $dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading from Samanantar's website\n",
    "!wget https://storage.googleapis.com/samanantar-public/V0.2/data/en2indic/en-hi.zip -P $dataset_path\n",
    "!unzip $dataset_path/en-hi.zip -d $dataset_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing and importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install seaborn\n",
    "!pip install wordcloud\n",
    "!pip install inltk\n",
    "!pip install indic-nlp-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One time setup for inltk\n",
    "from inltk.inltk import setup\n",
    "setup('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.precision\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring variables with paths to the dataset\n",
    "dataset_root = dataset_path + \"en-hi/\"\n",
    "english_set = dataset_root + \"train.en\"\n",
    "hindi_set = dataset_root + \"train.hi\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading English dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(english_set) as f:\n",
    "    X = f.readlines()\n",
    "\n",
    "print(\"Total Number of English Sentences are : \", len(X))\n",
    "\n",
    "print(\"\\nHere are some Sample English Sentences\\n\")\n",
    "\n",
    "for i, sentence in enumerate(X[1:5]):\n",
    "    print(\"Sentence Number \", i+1, \" : \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Hindi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hindi_set) as f:\n",
    "    Y = f.readlines()\n",
    "\n",
    "print(\"Total Number of Hindi Sentences are : \", len(Y))\n",
    "print(\"\\nHere are the corresponding Hindi Translations\\n\")\n",
    "\n",
    "for i, sentence in enumerate(Y[1:5]):\n",
    "    print(\"Sentence Number \", i+1, \" : \", sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary statistical analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of sentence lengths\n",
    "\n",
    "The maximum input sequence length for the transformer models has to be fixed. In order to deduce the maximum input sequence length that we can use for training the model, we will calculate the maximum, minimum and average length of the sentences in both the English and the Hindi corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_set_lengths = []\n",
    "hindi_set_lengths = []\n",
    "\n",
    "for eng_sentence in X:\n",
    "    english_set_lengths.append(len(eng_sentence))\n",
    "\n",
    "for hin_sentence in Y:\n",
    "    hindi_set_lengths.append(len(hin_sentence))\n",
    "\n",
    "print(\"The Maximum Length of a Single English sentence is %d, The Minimum length is %d and the Average is around %f\" %(\n",
    "    max(english_set_lengths), min(english_set_lengths), np.mean(english_set_lengths)))\n",
    "\n",
    "print(\"The Maximum Length of a Single Hindi sentence is %d, The Minimum length is %d and the Average is around %f\" %(\n",
    "    max(hindi_set_lengths), min(hindi_set_lengths), np.mean(hindi_set_lengths)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of lengths vs corresponding sentence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [100, 200, 400, 500, 750, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "len_count = []\n",
    "\n",
    "for j in range(len(lengths)):\n",
    "    if j == 0:\n",
    "        len_count.append(len([i for i in english_set_lengths if (i <= lengths[j])]))\n",
    "    else:\n",
    "        len_count.append(len([i for i in english_set_lengths if (i <= lengths[j] and i > lengths[j-1])]) + len_count[j-1])\n",
    "\n",
    "percentage = list(np.array(len_count)/len(X))\n",
    "\n",
    "distribution = pd.DataFrame(list(zip(lengths,len_count, percentage)), columns = ['Lengths', 'Count (<=)', 'Percentage of Dataset'])\n",
    "distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of English sentences with length < 5000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_u5000 = [i for i in english_set_lengths if i < 5000]\n",
    "print(\"Number of sentences with length < 5000 : \", len(eng_u5000))\n",
    "\n",
    "eng_df = pd.DataFrame(eng_u5000, columns =['Length'])\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.histplot(data = eng_df['Length'], kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of English sentences with length < 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_u1000 = [i for i in english_set_lengths if i < 1000]\n",
    "print(\"Number of sentences with length < 1000 : \", len(eng_u1000))\n",
    "\n",
    "eng_df2 = pd.DataFrame(eng_u1000, columns =['Length'])\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.histplot(data = eng_df2['Length'], kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Hindi sentences with length < 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hin_u2000 = [i for i in hindi_set_lengths if i < 2000]\n",
    "print(\"Number of Hindi sentences with length < 2000 : \", len(hin_u2000))\n",
    "\n",
    "hin_df = pd.DataFrame(hin_u2000, columns =['Length'])\n",
    "\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "sns.histplot(data = hin_df['Length'], kde = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Samanantar dataset provides untokenized and deduplicated data. These sentences cannot be passed directly as inputs to the model. As with every deep learning training regime, we will pre-process the dataset before training the model. The preprocessing steps include - lowercasing, length filtering, normalization, and tokenization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we normalize text, we attempt to reduce its randomness, bringing it closer to a predefined “standard”. This helps us to reduce the amount of different information that the computer has to deal with, and therefore improves efficiency. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normalizing English set, we will use the normalization script developed by [moses](https://github.com/moses-smt/mosesdecoder). <br>\n",
    "This script removes extra spaces, normalizes the unicode for punctuations, handles psuedo spaces and different types of quotation styles (French, German, Spanish)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_sentences = pd.DataFrame(X, columns = ['Text'])\n",
    "english_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl normalize-punctuation.perl -l en < $dataset_path/en-hi/train.en > $dataset_path/en-hi/train.normalized.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_set_normalized = dataset_root + '/train.normalized.en'\n",
    "\n",
    "with open(english_set_normalized) as f:\n",
    "    X_norm = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hindi set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normalizing Hindi set, we will use the Devanagari normalizer provided by [IndicNLP library](https://indic-nlp-library.readthedocs.io/en/latest/indicnlp.normalize.html). <br>\n",
    "This function replaces the composite characters containing nuktas by their decomposed form, replace pipe character '|' by poorna virama character, and replace colon ':' by visarga."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "factory=IndicNormalizerFactory()\n",
    "normalizer=factory.get_normalizer('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step might take a few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_normalized = []\n",
    "for index, input_text in enumerate(Y):\n",
    "    output_text=normalizer.normalize(input_text)\n",
    "    Y_normalized.append(output_text)\n",
    "\n",
    "print(\"Sample Sentences : \\n\")\n",
    "print(Y_normalized[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_set_normalized = dataset_root + '/train-normalized.hi'\n",
    "\n",
    "with open(hindi_set_normalized, \"w\") as output:\n",
    "    output.writelines(Y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(hindi_set_normalized) as f:\n",
    "    Y_normalized = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The English set is lowercased to reduce the token vocabulary size. The vocabulary size is the count of unique tokens that are used to train an NLP model. Lowercasing the English samples combines the two tokens “European” and “european”, thus decreasing the vocabulary size. \n",
    "\n",
    "Lowercasing is ignored for Hindi set because it does not affect any token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower = [sent.lower() for sent in X_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_lower[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform length on the English and Hindi set with a filter length of 1000 words. <br>\n",
    "Sentences that are longer than the maximum input sequence length are typically rejected or chopped short. Because the word order changes from subject-verb-object to subject-object-verb in En-Hi translation, cutting a sentence midway could result in a loss of meaning. <br>\n",
    "As a result, length filtering aids in the removal of these lengthy sentences and helps the model to learn from meaningful samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_filtered = []\n",
    "Y_filtered = []\n",
    "# Set the filtering length\n",
    "filtering_length = 1000\n",
    "\n",
    "for index, sentence in enumerate(X_lower):\n",
    "    if len(sentence) < filtering_length and len(Y_normalized[index]) < filtering_length:\n",
    "        X_filtered.append(sentence)\n",
    "        Y_filtered.append(Y_normalized[index])\n",
    "\n",
    "if len(X_filtered) == len(Y_filtered):\n",
    "    print(\"Finally, the number of sentences are : \", len(X_filtered))\n",
    "\n",
    "print(\"\\nHere are some sample sentences : \\n\")\n",
    "for i in range(2):\n",
    "    print(X_filtered[i])\n",
    "    print(Y_filtered[i], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_set_filtered = dataset_root + \"train.filtered_{0}.en\".format(filtering_length)\n",
    "%env english_set_filtered = {english_set_filtered}\n",
    "hindi_set_filtered = dataset_root + \"train.filtered_{0}.hi\".format(filtering_length)\n",
    "\n",
    "with open(english_set_filtered, \"w\") as output:\n",
    "    output.writelines(X_filtered)\n",
    "    \n",
    "with open(hindi_set_filtered, \"w\") as output:\n",
    "    output.writelines(Y_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization for data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is used to split the input sentence into a list of tokens; where a token is a word or a part of a sentence. There are a variety of commercial tokenizers available, each with its own set of tokenization principles.\n",
    "\n",
    "When the same input is fed to multiple tokenizers, the output can vary based on the operating principles of each tokenizer. \n",
    "There are different methods to tokenize a sentence such as:\n",
    "- white space tokenization - a sentence is split into tokens on white spaces \n",
    "- dictionary-based tokenization - tokens are found based on existing tokens in a dictionary\n",
    "- rule-based tokenization - the rules are based on grammar for a particular language and \n",
    "- sub-word tokenization - the less frequent words are split into subwords.\n",
    "\n",
    "The table below summarizes the tokenizers that we evaluated.\n",
    "\n",
    "| English      | Hindi |\n",
    "| ----------- | ----------- |\n",
    "| Moses      | IndicNLP       |\n",
    "| OpenNMT   | iNLTK        |\n",
    "| SentencePiece   | Moses        |\n",
    "| NLTK   | OpenNMT        |\n",
    "| Gruut   | CLTK        |\n",
    "\n",
    "The effect of various source and target side tokenizers can be studied using the BLEU score of the trained model on the validation dataset. In order to find the best performing pair of tokenizers, we experimented with different combinations of the tokenizers mentioned above for English and Hindi and inferred that Moses-Moses works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English set tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to save the tokenized dataset\n",
    "english_set_tokenized = dataset_root + \"train.filtered_{0}.tokenized_moses.en\".format(filtering_length)\n",
    "%env english_set_tokenized = {english_set_tokenized}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step might take a few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!perl tokenizer.perl -l en -no-escape < $english_set_filtered > $english_set_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hindi set tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import sentence_tokenize\n",
    "from indicnlp.tokenize import indic_tokenize  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step might take a few minutes to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tokenized = Y_filtered.copy()\n",
    "\n",
    "for index2, indic_string in enumerate(Y_filtered):\n",
    "    Y_tokenized[index2] = \" \".join(indic_tokenize.trivial_tokenize(indic_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to save the tokenized dataset\n",
    "hindi_set_tokenized = dataset_root + \"train.filtered_{0}.tokenized_indicnlp.hi\".format(filtering_length)\n",
    "\n",
    "with open(hindi_set_tokenized, \"w\") as output:\n",
    "    output.writelines(Y_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparision of Hindi tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the output of 2 diiferent tokenizers - IndicNLP and iNLTK - to better understand these tokenization algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = Y[1723]\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indic NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp.tokenize import indic_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_indicnlp = indic_tokenize.trivial_tokenize(string)\n",
    "print(len(tokens_indicnlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### iNLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inltk.inltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_inltk = tokenize(string ,'hi') \n",
    "print(len(tokens_inltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(tokens_indicnlp).intersection(set(tokens_inltk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_indicnlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_inltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_inltk_filt = [e[1:] if e[0] == '▁' else e for e in tokens_inltk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_inltk_filt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_indicnlp[:-1] == tokens_inltk_filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection = set(tokens_indicnlp).intersection(set(tokens_inltk_filt))\n",
    "print(len(intersection))\n",
    "print(intersection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, the iNLTK tokenizer splits the words गुलामों and सलूक into गुलाम, ो and  सल, ूक respectively in contrast to the IndicNLP tokenizer. The IndicNLP Tokenizer also adds the newline character \"\\n\" at the end of each tokenized sentence. Thus, the same input sentence has two different tokenized outputs with varying sequence lengths of 15 and 16 respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tokenized = []\n",
    "with open(english_set_tokenized) as f:\n",
    "    X_tokenized = f.readlines()\n",
    "\n",
    "Y_tokenized = []\n",
    "with open(hindi_set_tokenized) as f:\n",
    "    Y_tokenized = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English Tokenized : \")\n",
    "print(X_tokenized[121].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hindi Tokenized : \")\n",
    "print(Y_tokenized[121].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word cloud is a visual representation of text data. It emphasizes the importance of each word using font size and color. The font size depicts the relative frequency of occurrence of each word in the dataset. Let’s look at the word cloud representation of both the English and the Hindi text of our corpora.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "\n",
    "english_sentences = \"\"\n",
    "for sentence in X_tokenized:\n",
    "    english_sentences += sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step might take a few minutes to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(width=1200, height=1160, max_words=500,colormap=\"Dark2\").generate(english_sentences)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"English wordcloud\",fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hindi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_sentences = \"\"\n",
    "for sentence in Y_tokenized:\n",
    "    hindi_sentences += sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** This step might more than 10 minutes to complete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(font_path='Lohit-Devanagari.ttf',width=1200, height=1160, max_words=500,colormap=\"Dark2\").generate(hindi_sentences)\n",
    "\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Hindi Wordcloud\",fontsize=13)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word clouds above, you will observe the frequently occurring words in English and Hindi (excluding stopwords like “a”, “an”, “is”, “the\" etc.). The correspondence between the English and Hindi words can also be examined. For example, \"india\" and \"भारत\" both have a similar frequency in the corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the stop words from both the language's set and plot the frequency of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokens = []\n",
    "for sent in X_tokenized:\n",
    "    final_tokens.extend(sent.split())\n",
    "\n",
    "print(len(final_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_these = set(stopwords.words('english') + list(string.punctuation) + list(string.digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_text = [w for w in final_tokens if not w in remove_these]\n",
    "fdist_filtered = FreqDist(filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tokens : \", fdist_filtered.N())\n",
    "print(\"Total number of tokens : \", len(fdist_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_filtered.plot(30,title='Frequency distribution for 30 most common tokens (excluding stopwords)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_tokens_hindi = []\n",
    "for sent in Y_tokenized:\n",
    "    final_tokens_hindi.extend(sent.split())\n",
    "\n",
    "print(len(final_tokens_hindi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist_hin = FreqDist(final_tokens_hindi)\n",
    "fdist_hin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of tokens : \", fdist_hin.N())\n",
    "print(\"Total number of unique tokens : \", len(fdist_hin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindi_tokens = {}\n",
    "\n",
    "for token in final_tokens_hindi:\n",
    "    if token in hindi_tokens:\n",
    "        hindi_tokens[token] += 1\n",
    "    else:\n",
    "        hindi_tokens[token] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_hindi_tokens = sorted(hindi_tokens.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_plot = {}\n",
    "for x in sorted_hindi_tokens[0:30]:\n",
    "    dict_for_plot[x[0]] = x[1]\n",
    "\n",
    "dict_for_plot.keys()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.font_manager import FontProperties\n",
    "fp = FontProperties(fname='Lohit-Devanagari.ttf')\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15, 6))\n",
    "idx = np.asarray([i for i in range(30)])\n",
    "\n",
    "ax.bar(idx, [val for key,val in sorted(dict_for_plot.items(), key=lambda x: x[1], reverse=True)], width=1)\n",
    "ax.set_xticks(idx)\n",
    "ax.set_xticklabels(list(dict_for_plot.keys()), font=fp)\n",
    "ax.set_xlabel('Hindi tokens')\n",
    "ax.set_ylabel('Number of occurrences')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a 95:5 train-val split to train our models, which will then be evaluated on standard test sets like WAT and WMT.\n",
    "\n",
    "We are setting out 5% of the dataset (8.4M samples) for validation i.e. 0.42M samples, which is quite a lot.<br>\n",
    "Thus, we recommend you to play with the `val_ratio` to increase the training set size which might improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ratio = 0.05\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_tokenized, Y_tokenized, test_size=val_ratio, random_state=1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_root + \"/final_train_norm_lf_{0}_tk_moses.en\".format(filtering_length), \"w\") as output:\n",
    "    output.writelines(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_root + \"/final_train_norm_lf_{0}_tk_indicnlp.hi\".format(filtering_length), \"w\") as output:\n",
    "    output.writelines(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_root + \"/final_val_norm_lf_{0}_tk_moses.en\".format(filtering_length), \"w\") as output:\n",
    "    output.writelines(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dataset_root + \"/final_val_norm_lf_{0}_tk_indicnlp.hi\".format(filtering_length), \"w\") as output:\n",
    "    output.writelines(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data analysis and preparation is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
