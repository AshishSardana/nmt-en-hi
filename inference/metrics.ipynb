{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c73f0281",
   "metadata": {},
   "source": [
    "# Testing the NMT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fbc3d5",
   "metadata": {},
   "source": [
    "Congratulations! You have successfully trained your first NMT model using NVIDIA NeMo. To generate outputs for your input sentences, use the trained `.nemo model` for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532ef8ac-0bec-4e56-a1c8-0485669c187c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Change the path to where the .nemo model is saved\n",
    "path_to_nemo_model = \"../model/AAYNBase.nemo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570e4c5-a376-46a9-a89b-0e68bc57217a",
   "metadata": {},
   "source": [
    "## Inference on a sample file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b326ef60",
   "metadata": {},
   "source": [
    "We have a sample `test.txt` that contains a few English sentences that we can try to translate with our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9fad2b-5d18-4b14-8821-67669716ca2d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!cat test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b9045b",
   "metadata": {},
   "source": [
    "To generate predictions, we'll use the `nmt_transformer_infer.py` and specify the path to the nemo model and the sample file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099358d2-e55e-4805-8c5a-392e3ff42bbd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!python nmt_transformer_infer.py --model=$path_to_nemo_model --srctext=test.txt --tgtout=test.output --beam_size=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8bbb38",
   "metadata": {},
   "source": [
    "Let's look at the translated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed0ba1f-4ed5-451f-9bba-5cbb27755074",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!cat test.output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13276561-b2a4-4eaa-a7a8-383e02d6dcf4",
   "metadata": {},
   "source": [
    "## Downloading test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca8291-2ae5-4013-ac16-1aaa6df8294f",
   "metadata": {},
   "source": [
    "Manually download the dataset from [Samanantar's website](https://indicnlp.ai4bharat.org/samanantar/#mirror-links).\n",
    "The mirrored link from [google drive](https://drive.google.com/drive/folders/1hR-8Mc7qQWsZAC-cw-nUqG8_OCqCdq-b?usp=sharing) is most stable.\n",
    "\n",
    " Download the `benchmarks.zip` file to a local directory and extract the contents. The folders of interest are `benchmarks\\wat2020-devtest\\en-hi`, `benchmarks\\wat2021-devtest` and `benchmarks\\wmt-news\\en-hi`. These folders contain the source sentences along with their ground truth translations. The ground truth translations are used to calculate the metric, which is used to rate the quality of the translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9337c94d-c400-4ff9-b0b8-8bf69c85fcdf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Change the path to where the benchmarks folder in downloaded and unziped\n",
    "path_to_benchmarks_folder = 'benchmarks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808dc040-fc1d-43f5-b768-5c79039bbcca",
   "metadata": {},
   "source": [
    "## Calculating sacreBLEU on test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35836207",
   "metadata": {},
   "source": [
    "We can use the same `nmt_transformer_infer.py` script to generate predictions for the 3 test datasets - WAT2020, WAT2021, and WMT.\n",
    "\n",
    "Make sure that the path to the benchmarks folder is set correctly!\n",
    "\n",
    " Here, we use the Bilingual Evaluation Understudy or BLEU score. The BLEU score works by counting matching n-grams in the candidate and reference text, where each token is represented by a 1-gram or unigram, and each word pair is represented by a bigram comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6777645-d536-4224-a6fe-e599dc923707",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generating predictions for WAT2020 test set\n",
    "!python nmt_transformer_infer.py --model=$path_to_nemo_model --srctext=$path_to_benchmarks_folder/wat2020-devtest/en-hi/test.en --tgtout=wat2020_test_out.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bac89d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generating predictions for WAT2021 test set\n",
    "!python nmt_transformer_infer.py --model=$path_to_nemo_model --srctext=$path_to_benchmarks_folder/wat2021-devtest/test.en --tgtout=wat2021_test_out.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af37d8",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Generating predictions for WMT test set\n",
    "!python nmt_transformer_infer.py --model=$path_to_nemo_model --srctext=$path_to_benchmarks_folder/wmt-news/en-hi/test.en --tgtout=wmt-news_test_out.pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587c3e34-69d2-4a2e-918e-b7f8e5c35753",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Installing sacrebleu library\n",
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d758f92c-6ccf-4482-b650-d8a1643ceadc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating sacreBLEU for WAT2020\n",
    "!sacrebleu $path_to_benchmarks_folder/wat2020-devtest/en-hi/test.hi -i wat2020_test_out.pre --score-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d353257-02f7-48fd-800b-5d078db98b6c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating sacreBLEU for WAT2021\n",
    "!sacrebleu $path_to_benchmarks_folder/wat2021-devtest/test.hi -i wat2021_test_out.pre --score-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be81940-2a70-4e9e-bcb5-0955a1a5af21",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating sacreBLEU for WMT-News\n",
    "!sacrebleu $path_to_benchmarks_folder/wmt-news/en-hi/test.hi -i wmt-news_test_out.pre --score-only"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
