{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: dataset_root=../../dataset/en-hi\n",
      "env: english_set_tokenized_train=../../dataset/en-hi/final_train_norm_lf_1000_tk_moses.en\n",
      "env: english_set_tokenized_val=../../dataset/en-hi/final_val_norm_lf_1000_tk_moses.en\n",
      "env: hindi_set_tokenized_train=../../dataset/en-hi/final_train_norm_lf_1000_tk_indicnlp.hi\n",
      "env: hindi_set_tokenized_val=../../dataset/en-hi/final_val_norm_lf_1000_tk_indicnlp.hi\n",
      "env: english_hindi_set_mixed=../../dataset/en-hi/train.hi.en\n"
     ]
    }
   ],
   "source": [
    "dataset_root = '../../dataset/en-hi'\n",
    "%env dataset_root = {dataset_root}\n",
    "%env english_set_tokenized_train = {dataset_root}/final_train_norm_lf_1000_tk_moses.en\n",
    "%env english_set_tokenized_val = {dataset_root}/final_val_norm_lf_1000_tk_moses.en\n",
    "%env hindi_set_tokenized_train = {dataset_root}/final_train_norm_lf_1000_tk_indicnlp.hi\n",
    "%env hindi_set_tokenized_val = {dataset_root}/final_val_norm_lf_1000_tk_indicnlp.hi\n",
    "%env english_hindi_set_mixed = {dataset_root}/train.hi.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training shared BPE tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenating both the datasets for training shared tokenizer\n",
    "! cat $english_set_tokenized_train $hindi_set_tokenized_train > $english_hindi_set_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffling the dataset\n",
    "!shuf $english_hindi_set_mixed -o $english_hindi_set_mixed.shuf\n",
    "!rm $english_hindi_set_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training parameters\n",
      "  input: ../../dataset/en-hi/train.hi.en.shuf\n",
      "  model: preproc/tokenizer.BPE.32000.model\n",
      "  vocab_size: 32000\n",
      "  n_threads: 8\n",
      "  character_coverage: 0.999\n",
      "  pad: 0\n",
      "  unk: 1\n",
      "  bos: 2\n",
      "  eos: 3\n",
      "\n",
      "reading file...\n",
      "learning bpe...\n",
      "number of unique characters in the training data: 2874\n",
      "number of deleted characters: 2764\n",
      "number of unique characters left: 110\n",
      "id: 1000=122+311              freq: 148556      subword: ▁these=▁the+se\n",
      "id: 2000=580+184              freq: 60885       subword: ▁partic=▁part+ic\n",
      "id: 3000=123+398              freq: 36705       subword: ▁मंद=▁म+ंद\n",
      "id: 4000=144+234              freq: 25372       subword: ▁bad=▁b+ad\n",
      "id: 5000=190+7                freq: 18524       subword: ilt=il+t\n",
      "id: 6000=42+12                freq: 14388       subword: ws=w+s\n",
      "id: 7000=2258+410             freq: 11478       subword: ▁लड़की=▁लड़+की\n",
      "id: 8000=276+148              freq: 9472        subword: ▁जाया=▁जा+या\n",
      "id: 9000=2128+354             freq: 7906        subword: ounter=oun+ter\n",
      "id: 10000=121+13              freq: 6735        subword: ▁हक=▁ह+क\n",
      "id: 11000=10172+9652          freq: 5809        subword: ▁तेंदुलकर=▁तेंद+ुलकर\n",
      "id: 12000=450+181             freq: 5050        subword: ुमान=ुम+ान\n",
      "id: 13000=12160+1567          freq: 4429        subword: ▁फाउंडेशन=▁फाउंड+ेशन\n",
      "id: 14000=4771+48             freq: 3936        subword: ▁टैग=▁टै+ग\n",
      "id: 15000=294+1373            freq: 3509        subword: ▁asks=▁as+ks\n",
      "id: 16000=769+167             freq: 3155        subword: ▁himal=▁him+al\n",
      "id: 17000=503+60              freq: 2847        subword: गेड=गे+ड\n",
      "id: 18000=331+7681            freq: 2580        subword: ▁अपलोड=▁अप+लोड\n",
      "id: 19000=5199+1784           freq: 2347        subword: ▁walked=▁wal+ked\n",
      "id: 20000=1073+39             freq: 2150        subword: ▁नाव=▁ना+व\n",
      "id: 21000=616+314             freq: 1968        subword: ▁prith=▁pr+ith\n",
      "id: 22000=150+844             freq: 1822        subword: ▁mha=▁m+ha\n",
      "id: 23000=2115+218            freq: 1685        subword: ▁आमाल=▁आम+ाल\n",
      "id: 24000=521+23722           freq: 1563        subword: ▁सरस्वती=▁सर+स्वती\n",
      "id: 25000=1761+785            freq: 1453        subword: ▁blank=▁bl+ank\n",
      "id: 26000=30+5501             freq: 1357        subword: matic=m+atic\n",
      "id: 27000=4099+130            freq: 1271        subword: holder=hold+er\n",
      "id: 28000=12926+748           freq: 1189        subword: ▁expedition=▁exped+ition\n",
      "id: 29000=552+504             freq: 1114        subword: ▁कुंड=▁कु+ंड\n",
      "id: 30000=167+577             freq: 1049        subword: alous=al+ous\n",
      "id: 31000=4056+30057          freq: 985         subword: ▁रंगमंच=▁रंग+मंच\n",
      "model saved to: preproc/tokenizer.BPE.32000.model\n"
     ]
    }
   ],
   "source": [
    "!yttm bpe --data $english_hindi_set_mixed.shuf --model preproc/tokenizer.BPE.32000.model --vocab_size 32000 --coverage 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NMT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo W 2022-06-28 10:45:45 experimental:27] Module <class 'nemo.collections.nlp.data.language_modeling.megatron.megatron_batch_samplers.MegatronPretrainingRandomBatchSampler'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2022-06-28 10:45:45 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo I 2022-06-28 10:45:45 enc_dec_nmt:108] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2022-06-28 10:45:45 enc_dec_nmt:109] Config: name: AttentionIsAllYouNeed\n",
      "    model:\n",
      "      train_ds:\n",
      "        src_file_name: ../../dataset/en-hi/final_train_norm_lf_1000_tk_moses.en\n",
      "        tgt_file_name: ../../dataset/en-hi/final_train_norm_lf_1000_tk_indicnlp.hi\n",
      "        use_tarred_dataset: true\n",
      "        tar_files: null\n",
      "        metadata_file: null\n",
      "        lines_per_dataset_fragment: 1000000\n",
      "        num_batches_per_tarfile: 200\n",
      "        shard_strategy: scatter\n",
      "        tokens_in_batch: 12500\n",
      "        clean: true\n",
      "        max_seq_length: 512\n",
      "        min_seq_length: 1\n",
      "        cache_ids: false\n",
      "        cache_data_per_node: false\n",
      "        use_cache: false\n",
      "        shuffle: true\n",
      "        num_samples: -1\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        num_workers: 8\n",
      "        reverse_lang_direction: false\n",
      "        load_from_tarred_dataset: false\n",
      "        metadata_path: null\n",
      "        tar_shuffle_n: 100\n",
      "        n_preproc_jobs: -2\n",
      "        tar_file_prefix: parallel\n",
      "        concat_sampling_technique: temperature\n",
      "        concat_sampling_temperature: 5\n",
      "        concat_sampling_probabilities: null\n",
      "      validation_ds:\n",
      "        src_file_name: ../../dataset/en-hi/final_val_norm_lf_1000_tk_moses.en\n",
      "        tgt_file_name: ../../dataset/en-hi/final_val_norm_lf_1000_tk_indicnlp.hi\n",
      "        use_tarred_dataset: false\n",
      "        tar_files: null\n",
      "        metadata_file: null\n",
      "        lines_per_dataset_fragment: 1000000\n",
      "        num_batches_per_tarfile: 1000\n",
      "        shard_strategy: scatter\n",
      "        tokens_in_batch: 8192\n",
      "        clean: false\n",
      "        max_seq_length: 512\n",
      "        min_seq_length: 1\n",
      "        cache_ids: false\n",
      "        cache_data_per_node: false\n",
      "        use_cache: false\n",
      "        shuffle: false\n",
      "        num_samples: -1\n",
      "        drop_last: false\n",
      "        pin_memory: false\n",
      "        num_workers: 8\n",
      "        reverse_lang_direction: false\n",
      "        load_from_tarred_dataset: false\n",
      "        metadata_path: null\n",
      "        tar_shuffle_n: 100\n",
      "        n_preproc_jobs: -2\n",
      "        tar_file_prefix: parallel\n",
      "        concat_sampling_technique: temperature\n",
      "        concat_sampling_temperature: 5\n",
      "        concat_sampling_probabilities: null\n",
      "      optim:\n",
      "        name: adam\n",
      "        lr: 0.001\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        weight_decay: 0.0\n",
      "        sched:\n",
      "          name: InverseSquareRootAnnealing\n",
      "          min_lr: 0.0\n",
      "          last_epoch: -1\n",
      "          warmup_ratio: 0.1\n",
      "      encoder_tokenizer:\n",
      "        library: yttm\n",
      "        tokenizer_model: preproc/tokenizer.BPE.8192.model\n",
      "        vocab_size: 32000\n",
      "        bpe_dropout: null\n",
      "        vocab_file: null\n",
      "        special_tokens: null\n",
      "        training_sample_size: null\n",
      "        r2l: false\n",
      "      decoder_tokenizer:\n",
      "        library: yttm\n",
      "        tokenizer_model: preproc/tokenizer.BPE.8192.model\n",
      "        vocab_size: 32000\n",
      "        bpe_dropout: null\n",
      "        vocab_file: null\n",
      "        special_tokens: null\n",
      "        training_sample_size: null\n",
      "        r2l: false\n",
      "      encoder:\n",
      "        library: nemo\n",
      "        model_name: null\n",
      "        pretrained: false\n",
      "        max_sequence_length: 512\n",
      "        num_token_types: 0\n",
      "        embedding_dropout: 0.1\n",
      "        learn_positional_encodings: false\n",
      "        hidden_size: 512\n",
      "        num_layers: 6\n",
      "        inner_size: 2048\n",
      "        num_attention_heads: 8\n",
      "        ffn_dropout: 0.1\n",
      "        attn_score_dropout: 0.1\n",
      "        attn_layer_dropout: 0.1\n",
      "        hidden_act: relu\n",
      "        mask_future: false\n",
      "        pre_ln: false\n",
      "        pre_ln_final_layer_norm: true\n",
      "      decoder:\n",
      "        library: nemo\n",
      "        model_name: null\n",
      "        pretrained: false\n",
      "        max_sequence_length: 512\n",
      "        num_token_types: 0\n",
      "        embedding_dropout: 0.1\n",
      "        learn_positional_encodings: false\n",
      "        hidden_size: 512\n",
      "        inner_size: 2048\n",
      "        num_layers: 6\n",
      "        num_attention_heads: 8\n",
      "        ffn_dropout: 0.1\n",
      "        attn_score_dropout: 0.1\n",
      "        attn_layer_dropout: 0.1\n",
      "        hidden_act: relu\n",
      "        pre_ln: false\n",
      "        pre_ln_final_layer_norm: true\n",
      "      head:\n",
      "        num_layers: 1\n",
      "        activation: relu\n",
      "        log_softmax: true\n",
      "        dropout: 0.0\n",
      "        use_transformer_init: true\n",
      "      num_val_examples: 3\n",
      "      num_test_examples: 3\n",
      "      max_generation_delta: 5\n",
      "      label_smoothing: 0.1\n",
      "      beam_size: 4\n",
      "      len_pen: 0.6\n",
      "      src_language: en\n",
      "      tgt_language: de\n",
      "      find_unused_parameters: true\n",
      "      shared_tokenizer: true\n",
      "      multilingual: false\n",
      "      preproc_out_dir: preproc\n",
      "      validate_input_ids: true\n",
      "      shared_embeddings: false\n",
      "    trainer:\n",
      "      logger: false\n",
      "      checkpoint_callback: true\n",
      "      callbacks: null\n",
      "      default_root_dir: null\n",
      "      gradient_clip_val: 0.0\n",
      "      process_position: 0\n",
      "      num_nodes: 1\n",
      "      gpus: null\n",
      "      auto_select_gpus: false\n",
      "      tpu_cores: null\n",
      "      log_gpu_memory: null\n",
      "      progress_bar_refresh_rate: 1\n",
      "      enable_progress_bar: true\n",
      "      overfit_batches: 0.0\n",
      "      track_grad_norm: -1\n",
      "      check_val_every_n_epoch: 1\n",
      "      fast_dev_run: false\n",
      "      accumulate_grad_batches: 1\n",
      "      max_epochs: 1000\n",
      "      min_epochs: 1\n",
      "      max_steps: 150000\n",
      "      min_steps: null\n",
      "      limit_train_batches: 1.0\n",
      "      limit_val_batches: 1.0\n",
      "      limit_test_batches: 1.0\n",
      "      val_check_interval: 1.0\n",
      "      flush_logs_every_n_steps: 100\n",
      "      log_every_n_steps: 50\n",
      "      accelerator: gpu\n",
      "      sync_batchnorm: false\n",
      "      precision: 16\n",
      "      weights_summary: full\n",
      "      weights_save_path: null\n",
      "      num_sanity_val_steps: 2\n",
      "      resume_from_checkpoint: null\n",
      "      profiler: null\n",
      "      benchmark: false\n",
      "      deterministic: false\n",
      "      auto_lr_find: false\n",
      "      replace_sampler_ddp: true\n",
      "      detect_anomaly: false\n",
      "      terminate_on_nan: false\n",
      "      auto_scale_batch_size: false\n",
      "      prepare_data_per_node: true\n",
      "      amp_backend: native\n",
      "      amp_level: null\n",
      "      plugins: null\n",
      "      move_metrics_to_cpu: false\n",
      "      multiple_trainloader_mode: max_size_cycle\n",
      "      limit_predict_batches: 1.0\n",
      "      stochastic_weight_avg: false\n",
      "      gradient_clip_algorithm: norm\n",
      "      max_time: null\n",
      "      reload_dataloaders_every_n_epochs: 0\n",
      "      ipus: null\n",
      "      devices:\n",
      "      - 0\n",
      "      - 1\n",
      "      - 2\n",
      "      - 3\n",
      "      strategy: null\n",
      "      enable_checkpointing: false\n",
      "      enable_model_summary: true\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: results\n",
      "      name: AAYNBase\n",
      "      version: null\n",
      "      use_datetime_version: true\n",
      "      resume_if_exists: false\n",
      "      resume_past_end: false\n",
      "      resume_ignore_no_checkpoint: false\n",
      "      create_tensorboard_logger: true\n",
      "      summary_writer_kwargs: null\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs: null\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        filepath: null\n",
      "        dirpath: null\n",
      "        filename: null\n",
      "        monitor: val_sacreBLEU\n",
      "        verbose: true\n",
      "        save_last: true\n",
      "        save_top_k: 5\n",
      "        save_weights_only: false\n",
      "        mode: max\n",
      "        every_n_epochs: 1\n",
      "        prefix: null\n",
      "        postfix: .nemo\n",
      "        save_best_model: false\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: true\n",
      "        model_parallel_size: null\n",
      "      files_to_copy: []\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        reduction: mean\n",
      "        sync_cuda: false\n",
      "        buffer_size: 1\n",
      "      log_local_rank_0_only: false\n",
      "      log_global_rank_0_only: false\n",
      "    hydra:\n",
      "      run:\n",
      "        dir: .\n",
      "      job_logging:\n",
      "        root:\n",
      "          handlers: null\n",
      "    do_training: true\n",
      "    do_testing: false\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp.py:20: LightningDeprecationWarning: The `pl.plugins.training_type.ddp.DDPPlugin` is deprecated in v1.6 and will be removed in v1.8. Use `pl.strategies.ddp.DDPStrategy` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:312: LightningDeprecationWarning: Passing <nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f1d01aacfd0> `strategy` to the `plugins` flag in Trainer has been deprecated in v1.5 and will be removed in v1.7. Use `Trainer(strategy=<nemo.collections.nlp.parts.nlp_overrides.NLPDDPPlugin object at 0x7f1d01aacfd0>)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:151: LightningDeprecationWarning: Setting `Trainer(checkpoint_callback=True)` is deprecated in v1.5 and will be removed in v1.7. Please consider using `Trainer(enable_checkpointing=True)`.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:96: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=1)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:191: LightningDeprecationWarning: Setting `Trainer(weights_summary=full)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.model_summary.ModelSummary` with `max_depth` directly to the Trainer's `callbacks` argument instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:81: LightningDeprecationWarning: Setting `prepare_data_per_node` with the trainer flag is deprecated in v1.5.0 and will be removed in v1.7.0. Please set `prepare_data_per_node` in `LightningDataModule` and/or `LightningModule` directly instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:569: LightningDeprecationWarning: Trainer argument `terminate_on_nan` was deprecated in v1.5 and will be removed in 1.7. Please use `Trainer(detect_anomaly=True)` instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:61: LightningDeprecationWarning: Setting `Trainer(flush_logs_every_n_steps=100)` is deprecated in v1.5 and will be removed in v1.7. Please configure flushing in the logger instead.\n",
      "      rank_zero_deprecation(\n",
      "    \n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "[NeMo I 2022-06-28 10:45:45 tokenizer_utils:178] Getting YouTokenToMeTokenizer with model: preproc/tokenizer.BPE.8192.model with r2l: False.\n",
      "[NeMo I 2022-06-28 10:45:45 tokenizer_utils:178] Getting YouTokenToMeTokenizer with model: preproc/tokenizer.BPE.8192.model with r2l: False.\n",
      "[NeMo I 2022-06-28 10:45:45 preproc_mt_data:194] Creating tarred dataset for src: ../../dataset/en-hi/final_train_norm_lf_1000_tk_moses.en and tgt: ../../dataset/en-hi/final_train_norm_lf_1000_tk_indicnlp.hi\n",
      "[NeMo I 2022-06-28 10:45:45 preproc_mt_data:408] Tarred dataset detected: ['preproc/parallel.batches.tokens.12500.124.tar', 'preproc/parallel.batches.tokens.12500.67.tar', 'preproc/parallel.batches.tokens.12500.133.tar', 'preproc/parallel.batches.tokens.12500.115.tar', 'preproc/parallel.batches.tokens.12500.56.tar', 'preproc/parallel.batches.tokens.12500.166.tar', 'preproc/parallel.batches.tokens.12500.69.tar', 'preproc/parallel.batches.tokens.12500.95.tar', 'preproc/parallel.batches.tokens.12500.123.tar', 'preproc/parallel.batches.tokens.12500.135.tar', 'preproc/parallel.batches.tokens.12500.173.tar', 'preproc/parallel.batches.tokens.12500.146.tar', 'preproc/parallel.batches.tokens.12500.22.tar', 'preproc/parallel.batches.tokens.12500.161.tar', 'preproc/parallel.batches.tokens.12500.74.tar', 'preproc/parallel.batches.tokens.12500.120.tar', 'preproc/parallel.batches.tokens.12500.144.tar', 'preproc/parallel.batches.tokens.12500.109.tar', 'preproc/parallel.batches.tokens.12500.9.tar', 'preproc/parallel.batches.tokens.12500.47.tar', 'preproc/parallel.batches.tokens.12500.13.tar', 'preproc/parallel.batches.tokens.12500.132.tar', 'preproc/parallel.batches.tokens.12500.99.tar', 'preproc/parallel.batches.tokens.12500.50.tar', 'preproc/parallel.batches.tokens.12500.172.tar', 'preproc/parallel.batches.tokens.12500.176.tar', 'preproc/parallel.batches.tokens.12500.138.tar', 'preproc/parallel.batches.tokens.12500.159.tar', 'preproc/parallel.batches.tokens.12500.155.tar', 'preproc/parallel.batches.tokens.12500.70.tar', 'preproc/parallel.batches.tokens.12500.96.tar', 'preproc/parallel.batches.tokens.12500.79.tar', 'preproc/parallel.batches.tokens.12500.31.tar', 'preproc/parallel.batches.tokens.12500.130.tar', 'preproc/parallel.batches.tokens.12500.160.tar', 'preproc/parallel.batches.tokens.12500.137.tar', 'preproc/parallel.batches.tokens.12500.57.tar', 'preproc/parallel.batches.tokens.12500.145.tar', 'preproc/parallel.batches.tokens.12500.103.tar', 'preproc/parallel.batches.tokens.12500.16.tar', 'preproc/parallel.batches.tokens.12500.42.tar', 'preproc/parallel.batches.tokens.12500.152.tar', 'preproc/parallel.batches.tokens.12500.122.tar', 'preproc/parallel.batches.tokens.12500.139.tar', 'preproc/parallel.batches.tokens.12500.3.tar', 'preproc/parallel.batches.tokens.12500.171.tar', 'preproc/parallel.batches.tokens.12500.147.tar', 'preproc/parallel.batches.tokens.12500.24.tar', 'preproc/parallel.batches.tokens.12500.100.tar', 'preproc/parallel.batches.tokens.12500.36.tar', 'preproc/parallel.batches.tokens.12500.45.tar', 'preproc/parallel.batches.tokens.12500.5.tar', 'preproc/parallel.batches.tokens.12500.26.tar', 'preproc/parallel.batches.tokens.12500.134.tar', 'preproc/parallel.batches.tokens.12500.94.tar', 'preproc/parallel.batches.tokens.12500.17.tar', 'preproc/parallel.batches.tokens.12500.101.tar', 'preproc/parallel.batches.tokens.12500.82.tar', 'preproc/parallel.batches.tokens.12500.38.tar', 'preproc/parallel.batches.tokens.12500.86.tar', 'preproc/parallel.batches.tokens.12500.164.tar', 'preproc/parallel.batches.tokens.12500.151.tar', 'preproc/parallel.batches.tokens.12500.1.tar', 'preproc/parallel.batches.tokens.12500.15.tar', 'preproc/parallel.batches.tokens.12500.43.tar', 'preproc/parallel.batches.tokens.12500.28.tar', 'preproc/parallel.batches.tokens.12500.128.tar', 'preproc/parallel.batches.tokens.12500.6.tar', 'preproc/parallel.batches.tokens.12500.2.tar', 'preproc/parallel.batches.tokens.12500.141.tar', 'preproc/parallel.batches.tokens.12500.7.tar', 'preproc/parallel.batches.tokens.12500.117.tar', 'preproc/parallel.batches.tokens.12500.140.tar', 'preproc/parallel.batches.tokens.12500.54.tar', 'preproc/parallel.batches.tokens.12500.40.tar', 'preproc/parallel.batches.tokens.12500.30.tar', 'preproc/parallel.batches.tokens.12500.178.tar', 'preproc/parallel.batches.tokens.12500.114.tar', 'preproc/parallel.batches.tokens.12500.150.tar', 'preproc/parallel.batches.tokens.12500.84.tar', 'preproc/parallel.batches.tokens.12500.63.tar', 'preproc/parallel.batches.tokens.12500.34.tar', 'preproc/parallel.batches.tokens.12500.37.tar', 'preproc/parallel.batches.tokens.12500.125.tar', 'preproc/parallel.batches.tokens.12500.157.tar', 'preproc/parallel.batches.tokens.12500.81.tar', 'preproc/parallel.batches.tokens.12500.177.tar', 'preproc/parallel.batches.tokens.12500.48.tar', 'preproc/parallel.batches.tokens.12500.162.tar', 'preproc/parallel.batches.tokens.12500.4.tar', 'preproc/parallel.batches.tokens.12500.102.tar', 'preproc/parallel.batches.tokens.12500.121.tar', 'preproc/parallel.batches.tokens.12500.51.tar', 'preproc/parallel.batches.tokens.12500.118.tar', 'preproc/parallel.batches.tokens.12500.93.tar', 'preproc/parallel.batches.tokens.12500.175.tar', 'preproc/parallel.batches.tokens.12500.14.tar', 'preproc/parallel.batches.tokens.12500.29.tar', 'preproc/parallel.batches.tokens.12500.53.tar', 'preproc/parallel.batches.tokens.12500.20.tar', 'preproc/parallel.batches.tokens.12500.91.tar', 'preproc/parallel.batches.tokens.12500.75.tar', 'preproc/parallel.batches.tokens.12500.72.tar', 'preproc/parallel.batches.tokens.12500.35.tar', 'preproc/parallel.batches.tokens.12500.83.tar', 'preproc/parallel.batches.tokens.12500.119.tar', 'preproc/parallel.batches.tokens.12500.44.tar', 'preproc/parallel.batches.tokens.12500.61.tar', 'preproc/parallel.batches.tokens.12500.107.tar', 'preproc/parallel.batches.tokens.12500.127.tar', 'preproc/parallel.batches.tokens.12500.168.tar', 'preproc/parallel.batches.tokens.12500.154.tar', 'preproc/parallel.batches.tokens.12500.158.tar', 'preproc/parallel.batches.tokens.12500.76.tar', 'preproc/parallel.batches.tokens.12500.58.tar', 'preproc/parallel.batches.tokens.12500.59.tar', 'preproc/parallel.batches.tokens.12500.25.tar', 'preproc/parallel.batches.tokens.12500.80.tar', 'preproc/parallel.batches.tokens.12500.156.tar', 'preproc/parallel.batches.tokens.12500.68.tar', 'preproc/parallel.batches.tokens.12500.90.tar', 'preproc/parallel.batches.tokens.12500.89.tar', 'preproc/parallel.batches.tokens.12500.174.tar', 'preproc/parallel.batches.tokens.12500.12.tar', 'preproc/parallel.batches.tokens.12500.65.tar', 'preproc/parallel.batches.tokens.12500.78.tar', 'preproc/parallel.batches.tokens.12500.32.tar', 'preproc/parallel.batches.tokens.12500.18.tar', 'preproc/parallel.batches.tokens.12500.165.tar', 'preproc/parallel.batches.tokens.12500.126.tar', 'preproc/parallel.batches.tokens.12500.71.tar', 'preproc/parallel.batches.tokens.12500.111.tar', 'preproc/parallel.batches.tokens.12500.106.tar', 'preproc/parallel.batches.tokens.12500.97.tar', 'preproc/parallel.batches.tokens.12500.170.tar', 'preproc/parallel.batches.tokens.12500.23.tar', 'preproc/parallel.batches.tokens.12500.136.tar', 'preproc/parallel.batches.tokens.12500.153.tar', 'preproc/parallel.batches.tokens.12500.169.tar', 'preproc/parallel.batches.tokens.12500.49.tar', 'preproc/parallel.batches.tokens.12500.129.tar', 'preproc/parallel.batches.tokens.12500.116.tar', 'preproc/parallel.batches.tokens.12500.0.tar', 'preproc/parallel.batches.tokens.12500.73.tar', 'preproc/parallel.batches.tokens.12500.66.tar', 'preproc/parallel.batches.tokens.12500.10.tar', 'preproc/parallel.batches.tokens.12500.167.tar', 'preproc/parallel.batches.tokens.12500.27.tar', 'preproc/parallel.batches.tokens.12500.113.tar', 'preproc/parallel.batches.tokens.12500.11.tar', 'preproc/parallel.batches.tokens.12500.39.tar', 'preproc/parallel.batches.tokens.12500.21.tar', 'preproc/parallel.batches.tokens.12500.105.tar', 'preproc/parallel.batches.tokens.12500.112.tar', 'preproc/parallel.batches.tokens.12500.92.tar', 'preproc/parallel.batches.tokens.12500.88.tar', 'preproc/parallel.batches.tokens.12500.148.tar', 'preproc/parallel.batches.tokens.12500.163.tar', 'preproc/parallel.batches.tokens.12500.142.tar', 'preproc/parallel.batches.tokens.12500.77.tar', 'preproc/parallel.batches.tokens.12500.131.tar', 'preproc/parallel.batches.tokens.12500.110.tar', 'preproc/parallel.batches.tokens.12500.149.tar', 'preproc/parallel.batches.tokens.12500.8.tar', 'preproc/parallel.batches.tokens.12500.62.tar', 'preproc/parallel.batches.tokens.12500.55.tar', 'preproc/parallel.batches.tokens.12500.104.tar', 'preproc/parallel.batches.tokens.12500.64.tar', 'preproc/parallel.batches.tokens.12500.143.tar', 'preproc/parallel.batches.tokens.12500.98.tar', 'preproc/parallel.batches.tokens.12500.87.tar', 'preproc/parallel.batches.tokens.12500.108.tar', 'preproc/parallel.batches.tokens.12500.52.tar', 'preproc/parallel.batches.tokens.12500.41.tar', 'preproc/parallel.batches.tokens.12500.46.tar', 'preproc/parallel.batches.tokens.12500.85.tar', 'preproc/parallel.batches.tokens.12500.33.tar', 'preproc/parallel.batches.tokens.12500.60.tar', 'preproc/parallel.batches.tokens.12500.19.tar'] and will be used. Remove if reprocessing.\n",
      "[NeMo I 2022-06-28 10:45:45 preproc_mt_data:255] Using tarred dataset created in folder(s) ['preproc'] and metadata created at ['preproc/metadata.tokens.12500.json']\n",
      "[NeMo I 2022-06-28 10:45:45 exp_manager:287] Experiments will be logged at results/AAYNBase/2022-06-28_10-45-45\n",
      "[NeMo I 2022-06-28 10:45:45 exp_manager:661] TensorboardLogger has been set up\n",
      "[NeMo W 2022-06-28 10:45:45 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:2302: LightningDeprecationWarning: `Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\n",
      "      rank_zero_deprecation(\"`Trainer.weights_save_path` has been deprecated in v1.6 and will be removed in v1.8.\")\n",
      "    \n",
      "[NeMo W 2022-06-28 10:45:45 exp_manager:895] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 150000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n",
      "[NeMo I 2022-06-28 10:45:45 tokenizer_utils:178] Getting YouTokenToMeTokenizer with model: /workspace/translation/code/training/preproc/tokenizer.BPE.8192.model with r2l: False.\n",
      "[NeMo I 2022-06-28 10:45:45 tokenizer_utils:178] Getting YouTokenToMeTokenizer with model: /workspace/translation/code/training/preproc/tokenizer.BPE.8192.model with r2l: False.\n",
      "Created a temporary directory at /tmp/tmpdkw7p9bw\n",
      "Writing /tmp/tmpdkw7p9bw/_remote_module_non_sriptable.py\n",
      "[NeMo I 2022-06-28 10:45:46 mt_enc_dec_model:681] Updated the path of 0 tarred files\n",
      "[NeMo I 2022-06-28 10:45:46 mt_enc_dec_model:682] Loading from tarred dataset ['preproc/parallel.batches.tokens.12500.124.tar', 'preproc/parallel.batches.tokens.12500.67.tar', 'preproc/parallel.batches.tokens.12500.133.tar', 'preproc/parallel.batches.tokens.12500.115.tar', 'preproc/parallel.batches.tokens.12500.56.tar', 'preproc/parallel.batches.tokens.12500.166.tar', 'preproc/parallel.batches.tokens.12500.69.tar', 'preproc/parallel.batches.tokens.12500.95.tar', 'preproc/parallel.batches.tokens.12500.123.tar', 'preproc/parallel.batches.tokens.12500.135.tar', 'preproc/parallel.batches.tokens.12500.173.tar', 'preproc/parallel.batches.tokens.12500.146.tar', 'preproc/parallel.batches.tokens.12500.22.tar', 'preproc/parallel.batches.tokens.12500.161.tar', 'preproc/parallel.batches.tokens.12500.74.tar', 'preproc/parallel.batches.tokens.12500.120.tar', 'preproc/parallel.batches.tokens.12500.144.tar', 'preproc/parallel.batches.tokens.12500.109.tar', 'preproc/parallel.batches.tokens.12500.9.tar', 'preproc/parallel.batches.tokens.12500.47.tar', 'preproc/parallel.batches.tokens.12500.13.tar', 'preproc/parallel.batches.tokens.12500.132.tar', 'preproc/parallel.batches.tokens.12500.99.tar', 'preproc/parallel.batches.tokens.12500.50.tar', 'preproc/parallel.batches.tokens.12500.172.tar', 'preproc/parallel.batches.tokens.12500.176.tar', 'preproc/parallel.batches.tokens.12500.138.tar', 'preproc/parallel.batches.tokens.12500.159.tar', 'preproc/parallel.batches.tokens.12500.155.tar', 'preproc/parallel.batches.tokens.12500.70.tar', 'preproc/parallel.batches.tokens.12500.96.tar', 'preproc/parallel.batches.tokens.12500.79.tar', 'preproc/parallel.batches.tokens.12500.31.tar', 'preproc/parallel.batches.tokens.12500.130.tar', 'preproc/parallel.batches.tokens.12500.160.tar', 'preproc/parallel.batches.tokens.12500.137.tar', 'preproc/parallel.batches.tokens.12500.57.tar', 'preproc/parallel.batches.tokens.12500.145.tar', 'preproc/parallel.batches.tokens.12500.103.tar', 'preproc/parallel.batches.tokens.12500.16.tar', 'preproc/parallel.batches.tokens.12500.42.tar', 'preproc/parallel.batches.tokens.12500.152.tar', 'preproc/parallel.batches.tokens.12500.122.tar', 'preproc/parallel.batches.tokens.12500.139.tar', 'preproc/parallel.batches.tokens.12500.3.tar', 'preproc/parallel.batches.tokens.12500.171.tar', 'preproc/parallel.batches.tokens.12500.147.tar', 'preproc/parallel.batches.tokens.12500.24.tar', 'preproc/parallel.batches.tokens.12500.100.tar', 'preproc/parallel.batches.tokens.12500.36.tar', 'preproc/parallel.batches.tokens.12500.45.tar', 'preproc/parallel.batches.tokens.12500.5.tar', 'preproc/parallel.batches.tokens.12500.26.tar', 'preproc/parallel.batches.tokens.12500.134.tar', 'preproc/parallel.batches.tokens.12500.94.tar', 'preproc/parallel.batches.tokens.12500.17.tar', 'preproc/parallel.batches.tokens.12500.101.tar', 'preproc/parallel.batches.tokens.12500.82.tar', 'preproc/parallel.batches.tokens.12500.38.tar', 'preproc/parallel.batches.tokens.12500.86.tar', 'preproc/parallel.batches.tokens.12500.164.tar', 'preproc/parallel.batches.tokens.12500.151.tar', 'preproc/parallel.batches.tokens.12500.1.tar', 'preproc/parallel.batches.tokens.12500.15.tar', 'preproc/parallel.batches.tokens.12500.43.tar', 'preproc/parallel.batches.tokens.12500.28.tar', 'preproc/parallel.batches.tokens.12500.128.tar', 'preproc/parallel.batches.tokens.12500.6.tar', 'preproc/parallel.batches.tokens.12500.2.tar', 'preproc/parallel.batches.tokens.12500.141.tar', 'preproc/parallel.batches.tokens.12500.7.tar', 'preproc/parallel.batches.tokens.12500.117.tar', 'preproc/parallel.batches.tokens.12500.140.tar', 'preproc/parallel.batches.tokens.12500.54.tar', 'preproc/parallel.batches.tokens.12500.40.tar', 'preproc/parallel.batches.tokens.12500.30.tar', 'preproc/parallel.batches.tokens.12500.178.tar', 'preproc/parallel.batches.tokens.12500.114.tar', 'preproc/parallel.batches.tokens.12500.150.tar', 'preproc/parallel.batches.tokens.12500.84.tar', 'preproc/parallel.batches.tokens.12500.63.tar', 'preproc/parallel.batches.tokens.12500.34.tar', 'preproc/parallel.batches.tokens.12500.37.tar', 'preproc/parallel.batches.tokens.12500.125.tar', 'preproc/parallel.batches.tokens.12500.157.tar', 'preproc/parallel.batches.tokens.12500.81.tar', 'preproc/parallel.batches.tokens.12500.177.tar', 'preproc/parallel.batches.tokens.12500.48.tar', 'preproc/parallel.batches.tokens.12500.162.tar', 'preproc/parallel.batches.tokens.12500.4.tar', 'preproc/parallel.batches.tokens.12500.102.tar', 'preproc/parallel.batches.tokens.12500.121.tar', 'preproc/parallel.batches.tokens.12500.51.tar', 'preproc/parallel.batches.tokens.12500.118.tar', 'preproc/parallel.batches.tokens.12500.93.tar', 'preproc/parallel.batches.tokens.12500.175.tar', 'preproc/parallel.batches.tokens.12500.14.tar', 'preproc/parallel.batches.tokens.12500.29.tar', 'preproc/parallel.batches.tokens.12500.53.tar', 'preproc/parallel.batches.tokens.12500.20.tar', 'preproc/parallel.batches.tokens.12500.91.tar', 'preproc/parallel.batches.tokens.12500.75.tar', 'preproc/parallel.batches.tokens.12500.72.tar', 'preproc/parallel.batches.tokens.12500.35.tar', 'preproc/parallel.batches.tokens.12500.83.tar', 'preproc/parallel.batches.tokens.12500.119.tar', 'preproc/parallel.batches.tokens.12500.44.tar', 'preproc/parallel.batches.tokens.12500.61.tar', 'preproc/parallel.batches.tokens.12500.107.tar', 'preproc/parallel.batches.tokens.12500.127.tar', 'preproc/parallel.batches.tokens.12500.168.tar', 'preproc/parallel.batches.tokens.12500.154.tar', 'preproc/parallel.batches.tokens.12500.158.tar', 'preproc/parallel.batches.tokens.12500.76.tar', 'preproc/parallel.batches.tokens.12500.58.tar', 'preproc/parallel.batches.tokens.12500.59.tar', 'preproc/parallel.batches.tokens.12500.25.tar', 'preproc/parallel.batches.tokens.12500.80.tar', 'preproc/parallel.batches.tokens.12500.156.tar', 'preproc/parallel.batches.tokens.12500.68.tar', 'preproc/parallel.batches.tokens.12500.90.tar', 'preproc/parallel.batches.tokens.12500.89.tar', 'preproc/parallel.batches.tokens.12500.174.tar', 'preproc/parallel.batches.tokens.12500.12.tar', 'preproc/parallel.batches.tokens.12500.65.tar', 'preproc/parallel.batches.tokens.12500.78.tar', 'preproc/parallel.batches.tokens.12500.32.tar', 'preproc/parallel.batches.tokens.12500.18.tar', 'preproc/parallel.batches.tokens.12500.165.tar', 'preproc/parallel.batches.tokens.12500.126.tar', 'preproc/parallel.batches.tokens.12500.71.tar', 'preproc/parallel.batches.tokens.12500.111.tar', 'preproc/parallel.batches.tokens.12500.106.tar', 'preproc/parallel.batches.tokens.12500.97.tar', 'preproc/parallel.batches.tokens.12500.170.tar', 'preproc/parallel.batches.tokens.12500.23.tar', 'preproc/parallel.batches.tokens.12500.136.tar', 'preproc/parallel.batches.tokens.12500.153.tar', 'preproc/parallel.batches.tokens.12500.169.tar', 'preproc/parallel.batches.tokens.12500.49.tar', 'preproc/parallel.batches.tokens.12500.129.tar', 'preproc/parallel.batches.tokens.12500.116.tar', 'preproc/parallel.batches.tokens.12500.0.tar', 'preproc/parallel.batches.tokens.12500.73.tar', 'preproc/parallel.batches.tokens.12500.66.tar', 'preproc/parallel.batches.tokens.12500.10.tar', 'preproc/parallel.batches.tokens.12500.167.tar', 'preproc/parallel.batches.tokens.12500.27.tar', 'preproc/parallel.batches.tokens.12500.113.tar', 'preproc/parallel.batches.tokens.12500.11.tar', 'preproc/parallel.batches.tokens.12500.39.tar', 'preproc/parallel.batches.tokens.12500.21.tar', 'preproc/parallel.batches.tokens.12500.105.tar', 'preproc/parallel.batches.tokens.12500.112.tar', 'preproc/parallel.batches.tokens.12500.92.tar', 'preproc/parallel.batches.tokens.12500.88.tar', 'preproc/parallel.batches.tokens.12500.148.tar', 'preproc/parallel.batches.tokens.12500.163.tar', 'preproc/parallel.batches.tokens.12500.142.tar', 'preproc/parallel.batches.tokens.12500.77.tar', 'preproc/parallel.batches.tokens.12500.131.tar', 'preproc/parallel.batches.tokens.12500.110.tar', 'preproc/parallel.batches.tokens.12500.149.tar', 'preproc/parallel.batches.tokens.12500.8.tar', 'preproc/parallel.batches.tokens.12500.62.tar', 'preproc/parallel.batches.tokens.12500.55.tar', 'preproc/parallel.batches.tokens.12500.104.tar', 'preproc/parallel.batches.tokens.12500.64.tar', 'preproc/parallel.batches.tokens.12500.143.tar', 'preproc/parallel.batches.tokens.12500.98.tar', 'preproc/parallel.batches.tokens.12500.87.tar', 'preproc/parallel.batches.tokens.12500.108.tar', 'preproc/parallel.batches.tokens.12500.52.tar', 'preproc/parallel.batches.tokens.12500.41.tar', 'preproc/parallel.batches.tokens.12500.46.tar', 'preproc/parallel.batches.tokens.12500.85.tar', 'preproc/parallel.batches.tokens.12500.33.tar', 'preproc/parallel.batches.tokens.12500.60.tar', 'preproc/parallel.batches.tokens.12500.19.tar']\n",
      "[NeMo I 2022-06-28 10:45:46 machine_translation_dataset:388] All tarred dataset shards will be scattered evenly across all nodes.\n",
      "[NeMo W 2022-06-28 10:45:46 machine_translation_dataset:390] Number of shards in tarred dataset (179) is not divisible by number of distributed workers (4).\n",
      "[NeMo I 2022-06-28 10:45:46 machine_translation_dataset:396] Begin Index : 0\n",
      "[NeMo I 2022-06-28 10:45:46 machine_translation_dataset:397] End Index : 44\n",
      "[NeMo I 2022-06-28 10:45:46 machine_translation_dataset:399] Partitioning tarred dataset: process (0) taking shards [0, 44)\n",
      "[NeMo I 2022-06-28 10:45:46 data_preprocessing:486] Tokenizing dataset ../../dataset/en-hi/final_val_norm_lf_1000_tk_moses.en...\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 57852.54it/s]\n",
      "[NeMo I 2022-06-28 10:45:48 data_preprocessing:486] Tokenizing dataset ../../dataset/en-hi/final_val_norm_lf_1000_tk_indicnlp.hi...\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 54062.33it/s]\n",
      "[NeMo W 2022-06-28 10:45:52 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (GlobalAverageLossMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "[NeMo I 2022-06-28 10:45:53 enc_dec_nmt:127] \n",
      "    \n",
      "    ************** Model parameters and their sizes ***********\n",
      "encoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "encoder._embedding.layer_norm.weight torch.Size([512])\n",
      "encoder._embedding.layer_norm.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "decoder._embedding.layer_norm.weight torch.Size([512])\n",
      "decoder._embedding.layer_norm.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "log_softmax.mlp.layer0.bias torch.Size([8192])\n",
      "[NeMo I 2022-06-28 10:45:53 enc_dec_nmt:130] ***********************************************************\n",
      "    \n",
      "    \n",
      "Created a temporary directory at /tmp/tmpu_decy0q\n",
      "Writing /tmp/tmpu_decy0q/_remote_module_non_sriptable.py\n",
      "Tokenizing sentence:  95%|██████████▍| 144967/152364 [00:02<00:00, 63232.33it/s]Created a temporary directory at /tmp/tmpc3yvms_n\n",
      "Writing /tmp/tmpc3yvms_n/_remote_module_non_sriptable.py\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 57845.86it/s]\n",
      "Tokenizing sentence:  40%|████▊       | 60947/152364 [00:01<00:01, 62482.12it/s]Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 58344.35it/s]\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 56262.16it/s]\n",
      "Tokenizing sentence:  32%|███▊        | 48690/152364 [00:00<00:01, 60683.06it/s]Created a temporary directory at /tmp/tmpiqi4gi17\n",
      "Writing /tmp/tmpiqi4gi17/_remote_module_non_sriptable.py\n",
      "Tokenizing sentence:  67%|███████▍   | 102562/152364 [00:01<00:00, 56979.22it/s]encoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "encoder._embedding.layer_norm.weight torch.Size([512])\n",
      "encoder._embedding.layer_norm.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "decoder._embedding.layer_norm.weight torch.Size([512])\n",
      "decoder._embedding.layer_norm.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "log_softmax.mlp.layer0.bias torch.Size([8192])\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "Added key: store_based_barrier_key:1 to store for rank: 1\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 55056.47it/s]\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 56523.04it/s]\n",
      "Tokenizing sentence:   8%|▉           | 11495/152364 [00:00<00:02, 57077.08it/s]encoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "encoder._embedding.layer_norm.weight torch.Size([512])\n",
      "encoder._embedding.layer_norm.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "decoder._embedding.layer_norm.weight torch.Size([512])\n",
      "decoder._embedding.layer_norm.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "log_softmax.mlp.layer0.bias torch.Size([8192])\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Added key: store_based_barrier_key:1 to store for rank: 2\n",
      "Tokenizing sentence: 100%|███████████| 152364/152364 [00:02<00:00, 53615.40it/s]\n",
      "encoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "encoder._embedding.layer_norm.weight torch.Size([512])\n",
      "encoder._embedding.layer_norm.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.0.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.1.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.2.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.3.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.4.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "encoder._encoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "encoder._encoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_in.bias torch.Size([2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "encoder._encoder.layers.5.second_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._embedding.token_embedding.weight torch.Size([8192, 512])\n",
      "decoder._embedding.layer_norm.weight torch.Size([512])\n",
      "decoder._embedding.layer_norm.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.0.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.0.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.0.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.1.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.1.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.1.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.2.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.2.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.2.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.3.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.3.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.3.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.4.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.4.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.4.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_1.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.first_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_2.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.query_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.key_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.value_net.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.weight torch.Size([512, 512])\n",
      "decoder._decoder.layers.5.second_sub_layer.out_projection.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.weight torch.Size([512])\n",
      "decoder._decoder.layers.5.layer_norm_3.bias torch.Size([512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.weight torch.Size([2048, 512])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_in.bias torch.Size([2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.weight torch.Size([512, 2048])\n",
      "decoder._decoder.layers.5.third_sub_layer.dense_out.bias torch.Size([512])\n",
      "log_softmax.mlp.layer0.bias torch.Size([8192])\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "Added key: store_based_barrier_key:1 to store for rank: 3\n",
      "Added key: store_based_barrier_key:1 to store for rank: 0\n",
      "Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo I 2022-06-28 10:46:14 modelPT:579] Optimizer config = Adam (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: [0.9, 0.98]\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        lr: 0.001\n",
      "        maximize: False\n",
      "        weight_decay: 0.0\n",
      "    )\n",
      "[NeMo I 2022-06-28 10:46:14 lr_scheduler:833] Scheduler \"<nemo.core.optim.lr_scheduler.InverseSquareRootAnnealing object at 0x7f1d01c692e0>\" \n",
      "    will be used during training (effective maximum steps = 150000) - \n",
      "    Parameters : \n",
      "    (min_lr: 0.0\n",
      "    last_epoch: -1\n",
      "    warmup_ratio: 0.1\n",
      "    max_steps: 150000\n",
      "    )\n",
      "\n",
      "    | Name                                                      | Type                     | Params\n",
      "---------------------------------------------------------------------------------------------------------\n",
      "0   | val_loss                                                  | GlobalAverageLossMetric  | 0     \n",
      "1   | encoder                                                   | TransformerEncoderNM     | 23.1 M\n",
      "2   | encoder._embedding                                        | TransformerEmbedding     | 4.2 M \n",
      "3   | encoder._embedding.token_embedding                        | Embedding                | 4.2 M \n",
      "4   | encoder._embedding.position_embedding                     | FixedPositionalEncoding  | 0     \n",
      "5   | encoder._embedding.layer_norm                             | LayerNorm                | 1.0 K \n",
      "6   | encoder._embedding.dropout                                | Dropout                  | 0     \n",
      "7   | encoder._encoder                                          | TransformerEncoder       | 18.9 M\n",
      "8   | encoder._encoder.layers                                   | ModuleList               | 18.9 M\n",
      "9   | encoder._encoder.layers.0                                 | TransformerEncoderBlock  | 3.2 M \n",
      "10  | encoder._encoder.layers.0.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "11  | encoder._encoder.layers.0.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "12  | encoder._encoder.layers.0.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "13  | encoder._encoder.layers.0.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "14  | encoder._encoder.layers.0.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "15  | encoder._encoder.layers.0.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "16  | encoder._encoder.layers.0.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "17  | encoder._encoder.layers.0.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "18  | encoder._encoder.layers.0.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "19  | encoder._encoder.layers.0.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "20  | encoder._encoder.layers.0.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "21  | encoder._encoder.layers.0.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "22  | encoder._encoder.layers.0.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "23  | encoder._encoder.layers.1                                 | TransformerEncoderBlock  | 3.2 M \n",
      "24  | encoder._encoder.layers.1.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "25  | encoder._encoder.layers.1.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "26  | encoder._encoder.layers.1.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "27  | encoder._encoder.layers.1.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "28  | encoder._encoder.layers.1.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "29  | encoder._encoder.layers.1.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "30  | encoder._encoder.layers.1.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "31  | encoder._encoder.layers.1.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "32  | encoder._encoder.layers.1.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "33  | encoder._encoder.layers.1.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "34  | encoder._encoder.layers.1.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "35  | encoder._encoder.layers.1.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "36  | encoder._encoder.layers.1.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "37  | encoder._encoder.layers.2                                 | TransformerEncoderBlock  | 3.2 M \n",
      "38  | encoder._encoder.layers.2.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "39  | encoder._encoder.layers.2.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "40  | encoder._encoder.layers.2.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "41  | encoder._encoder.layers.2.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "42  | encoder._encoder.layers.2.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "43  | encoder._encoder.layers.2.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "44  | encoder._encoder.layers.2.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "45  | encoder._encoder.layers.2.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "46  | encoder._encoder.layers.2.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "47  | encoder._encoder.layers.2.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "48  | encoder._encoder.layers.2.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "49  | encoder._encoder.layers.2.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "50  | encoder._encoder.layers.2.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "51  | encoder._encoder.layers.3                                 | TransformerEncoderBlock  | 3.2 M \n",
      "52  | encoder._encoder.layers.3.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "53  | encoder._encoder.layers.3.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "54  | encoder._encoder.layers.3.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "55  | encoder._encoder.layers.3.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "56  | encoder._encoder.layers.3.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "57  | encoder._encoder.layers.3.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "58  | encoder._encoder.layers.3.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "59  | encoder._encoder.layers.3.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "60  | encoder._encoder.layers.3.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "61  | encoder._encoder.layers.3.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "62  | encoder._encoder.layers.3.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "63  | encoder._encoder.layers.3.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "64  | encoder._encoder.layers.3.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "65  | encoder._encoder.layers.4                                 | TransformerEncoderBlock  | 3.2 M \n",
      "66  | encoder._encoder.layers.4.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "67  | encoder._encoder.layers.4.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "68  | encoder._encoder.layers.4.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "69  | encoder._encoder.layers.4.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "70  | encoder._encoder.layers.4.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "71  | encoder._encoder.layers.4.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "72  | encoder._encoder.layers.4.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "73  | encoder._encoder.layers.4.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "74  | encoder._encoder.layers.4.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "75  | encoder._encoder.layers.4.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "76  | encoder._encoder.layers.4.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "77  | encoder._encoder.layers.4.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "78  | encoder._encoder.layers.4.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "79  | encoder._encoder.layers.5                                 | TransformerEncoderBlock  | 3.2 M \n",
      "80  | encoder._encoder.layers.5.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "81  | encoder._encoder.layers.5.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "82  | encoder._encoder.layers.5.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "83  | encoder._encoder.layers.5.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "84  | encoder._encoder.layers.5.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "85  | encoder._encoder.layers.5.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "86  | encoder._encoder.layers.5.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "87  | encoder._encoder.layers.5.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "88  | encoder._encoder.layers.5.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "89  | encoder._encoder.layers.5.second_sub_layer                | PositionWiseFF           | 2.1 M \n",
      "90  | encoder._encoder.layers.5.second_sub_layer.dense_in       | Linear                   | 1.1 M \n",
      "91  | encoder._encoder.layers.5.second_sub_layer.dense_out      | Linear                   | 1.0 M \n",
      "92  | encoder._encoder.layers.5.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "93  | decoder                                                   | TransformerDecoderNM     | 29.4 M\n",
      "94  | decoder._embedding                                        | TransformerEmbedding     | 4.2 M \n",
      "95  | decoder._embedding.token_embedding                        | Embedding                | 4.2 M \n",
      "96  | decoder._embedding.position_embedding                     | FixedPositionalEncoding  | 0     \n",
      "97  | decoder._embedding.layer_norm                             | LayerNorm                | 1.0 K \n",
      "98  | decoder._embedding.dropout                                | Dropout                  | 0     \n",
      "99  | decoder._decoder                                          | TransformerDecoder       | 25.2 M\n",
      "100 | decoder._decoder.layers                                   | ModuleList               | 25.2 M\n",
      "101 | decoder._decoder.layers.0                                 | TransformerDecoderBlock  | 4.2 M \n",
      "102 | decoder._decoder.layers.0.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "103 | decoder._decoder.layers.0.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "104 | decoder._decoder.layers.0.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "105 | decoder._decoder.layers.0.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "106 | decoder._decoder.layers.0.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "107 | decoder._decoder.layers.0.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "108 | decoder._decoder.layers.0.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "109 | decoder._decoder.layers.0.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "110 | decoder._decoder.layers.0.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "111 | decoder._decoder.layers.0.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "112 | decoder._decoder.layers.0.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "113 | decoder._decoder.layers.0.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "114 | decoder._decoder.layers.0.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "115 | decoder._decoder.layers.0.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "116 | decoder._decoder.layers.0.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "117 | decoder._decoder.layers.0.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "118 | decoder._decoder.layers.0.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "119 | decoder._decoder.layers.0.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "120 | decoder._decoder.layers.0.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "121 | decoder._decoder.layers.0.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "122 | decoder._decoder.layers.0.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "123 | decoder._decoder.layers.1                                 | TransformerDecoderBlock  | 4.2 M \n",
      "124 | decoder._decoder.layers.1.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "125 | decoder._decoder.layers.1.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "126 | decoder._decoder.layers.1.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "127 | decoder._decoder.layers.1.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "128 | decoder._decoder.layers.1.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "129 | decoder._decoder.layers.1.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "130 | decoder._decoder.layers.1.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "131 | decoder._decoder.layers.1.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "132 | decoder._decoder.layers.1.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "133 | decoder._decoder.layers.1.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "134 | decoder._decoder.layers.1.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "135 | decoder._decoder.layers.1.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "136 | decoder._decoder.layers.1.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "137 | decoder._decoder.layers.1.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "138 | decoder._decoder.layers.1.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "139 | decoder._decoder.layers.1.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "140 | decoder._decoder.layers.1.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "141 | decoder._decoder.layers.1.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "142 | decoder._decoder.layers.1.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "143 | decoder._decoder.layers.1.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "144 | decoder._decoder.layers.1.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "145 | decoder._decoder.layers.2                                 | TransformerDecoderBlock  | 4.2 M \n",
      "146 | decoder._decoder.layers.2.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "147 | decoder._decoder.layers.2.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "148 | decoder._decoder.layers.2.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "149 | decoder._decoder.layers.2.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "150 | decoder._decoder.layers.2.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "151 | decoder._decoder.layers.2.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "152 | decoder._decoder.layers.2.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "153 | decoder._decoder.layers.2.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "154 | decoder._decoder.layers.2.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "155 | decoder._decoder.layers.2.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "156 | decoder._decoder.layers.2.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "157 | decoder._decoder.layers.2.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "158 | decoder._decoder.layers.2.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "159 | decoder._decoder.layers.2.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "160 | decoder._decoder.layers.2.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "161 | decoder._decoder.layers.2.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "162 | decoder._decoder.layers.2.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "163 | decoder._decoder.layers.2.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "164 | decoder._decoder.layers.2.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "165 | decoder._decoder.layers.2.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "166 | decoder._decoder.layers.2.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "167 | decoder._decoder.layers.3                                 | TransformerDecoderBlock  | 4.2 M \n",
      "168 | decoder._decoder.layers.3.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "169 | decoder._decoder.layers.3.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "170 | decoder._decoder.layers.3.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "171 | decoder._decoder.layers.3.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "172 | decoder._decoder.layers.3.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "173 | decoder._decoder.layers.3.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "174 | decoder._decoder.layers.3.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "175 | decoder._decoder.layers.3.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "176 | decoder._decoder.layers.3.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "177 | decoder._decoder.layers.3.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "178 | decoder._decoder.layers.3.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "179 | decoder._decoder.layers.3.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "180 | decoder._decoder.layers.3.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "181 | decoder._decoder.layers.3.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "182 | decoder._decoder.layers.3.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "183 | decoder._decoder.layers.3.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "184 | decoder._decoder.layers.3.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "185 | decoder._decoder.layers.3.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "186 | decoder._decoder.layers.3.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "187 | decoder._decoder.layers.3.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "188 | decoder._decoder.layers.3.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "189 | decoder._decoder.layers.4                                 | TransformerDecoderBlock  | 4.2 M \n",
      "190 | decoder._decoder.layers.4.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "191 | decoder._decoder.layers.4.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "192 | decoder._decoder.layers.4.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "193 | decoder._decoder.layers.4.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "194 | decoder._decoder.layers.4.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "195 | decoder._decoder.layers.4.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "196 | decoder._decoder.layers.4.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "197 | decoder._decoder.layers.4.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "198 | decoder._decoder.layers.4.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "199 | decoder._decoder.layers.4.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "200 | decoder._decoder.layers.4.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "201 | decoder._decoder.layers.4.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "202 | decoder._decoder.layers.4.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "203 | decoder._decoder.layers.4.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "204 | decoder._decoder.layers.4.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "205 | decoder._decoder.layers.4.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "206 | decoder._decoder.layers.4.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "207 | decoder._decoder.layers.4.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "208 | decoder._decoder.layers.4.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "209 | decoder._decoder.layers.4.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "210 | decoder._decoder.layers.4.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "211 | decoder._decoder.layers.5                                 | TransformerDecoderBlock  | 4.2 M \n",
      "212 | decoder._decoder.layers.5.layer_norm_1                    | LayerNorm                | 1.0 K \n",
      "213 | decoder._decoder.layers.5.first_sub_layer                 | MultiHeadAttention       | 1.1 M \n",
      "214 | decoder._decoder.layers.5.first_sub_layer.query_net       | Linear                   | 262 K \n",
      "215 | decoder._decoder.layers.5.first_sub_layer.key_net         | Linear                   | 262 K \n",
      "216 | decoder._decoder.layers.5.first_sub_layer.value_net       | Linear                   | 262 K \n",
      "217 | decoder._decoder.layers.5.first_sub_layer.out_projection  | Linear                   | 262 K \n",
      "218 | decoder._decoder.layers.5.first_sub_layer.attn_dropout    | Dropout                  | 0     \n",
      "219 | decoder._decoder.layers.5.first_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "220 | decoder._decoder.layers.5.layer_norm_2                    | LayerNorm                | 1.0 K \n",
      "221 | decoder._decoder.layers.5.second_sub_layer                | MultiHeadAttention       | 1.1 M \n",
      "222 | decoder._decoder.layers.5.second_sub_layer.query_net      | Linear                   | 262 K \n",
      "223 | decoder._decoder.layers.5.second_sub_layer.key_net        | Linear                   | 262 K \n",
      "224 | decoder._decoder.layers.5.second_sub_layer.value_net      | Linear                   | 262 K \n",
      "225 | decoder._decoder.layers.5.second_sub_layer.out_projection | Linear                   | 262 K \n",
      "226 | decoder._decoder.layers.5.second_sub_layer.attn_dropout   | Dropout                  | 0     \n",
      "227 | decoder._decoder.layers.5.second_sub_layer.layer_dropout  | Dropout                  | 0     \n",
      "228 | decoder._decoder.layers.5.layer_norm_3                    | LayerNorm                | 1.0 K \n",
      "229 | decoder._decoder.layers.5.third_sub_layer                 | PositionWiseFF           | 2.1 M \n",
      "230 | decoder._decoder.layers.5.third_sub_layer.dense_in        | Linear                   | 1.1 M \n",
      "231 | decoder._decoder.layers.5.third_sub_layer.dense_out       | Linear                   | 1.0 M \n",
      "232 | decoder._decoder.layers.5.third_sub_layer.layer_dropout   | Dropout                  | 0     \n",
      "233 | log_softmax                                               | TokenClassifier          | 4.2 M \n",
      "234 | log_softmax.dropout                                       | Dropout                  | 0     \n",
      "235 | log_softmax.mlp                                           | MultiLayerPerceptron     | 4.2 M \n",
      "236 | log_softmax.mlp.layer0                                    | Linear                   | 4.2 M \n",
      "237 | loss_fn                                                   | SmoothedCrossEntropyLoss | 0     \n",
      "238 | eval_loss_fn                                              | NLLLoss                  | 0     \n",
      "---------------------------------------------------------------------------------------------------------\n",
      "52.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "52.5 M    Total params\n",
      "105.075   Total estimated model params size (MB)\n",
      "Sanity Checking DataLoader 0:   0%|                       | 0/2 [00:00<?, ?it/s][NeMo W 2022-06-28 10:46:17 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/nemo/collections/nlp/modules/common/transformer/transformer_generators.py:363: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "      mems_ids = indices_i.unsqueeze(2).unsqueeze(3).repeat(1, 1, p_len - 1, hidden_size) // self.beam_size\n",
      "    \n",
      "[NeMo W 2022-06-28 10:46:17 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
      "                    not been set for this class (_ResultMetric). The property determines if `update` by\n",
      "                    default needs access to the full metric state. If this is not the case, significant speedups can be\n",
      "                    achieved and we recommend setting this to `False`.\n",
      "                    We provide an checking function\n",
      "                    `from torchmetrics.utilities import check_forward_no_full_state`\n",
      "                    that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
      "                    default for now) or if `full_state_update=False` can be used safely.\n",
      "                    \n",
      "      warnings.warn(*args, **kwargs)\n",
      "    \n",
      "Sanity Checking DataLoader 0: 100%|███████████████| 2/2 [00:02<00:00,  1.12it/s][NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:454] Dataset name: Validation, Dataloader index: 0, Set size: 968\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:457] Dataset name: Validation, Dataloader index: 0, Val Loss = 9.06799465921476\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:460] Dataset name: Validation, Dataloader index: 0, Sacre BLEU = 0.0\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:463] Dataset name: Validation, Dataloader index: 0, Translation Examples:\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:468]     E̲x̲a̲m̲p̲l̲e̲ ̲0̲:\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:469]     Input:        about 700 killed\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:470]     Prediction:   dition मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी मिलेगी\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:471]     Ground Truth: 700 लोगों को मरवाया\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:468]     E̲x̲a̲m̲p̲l̲e̲ ̲1̲:\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:469]     Input:        the family court\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:470]     Prediction:   ्ल्ल्ल्ल्ल्ल्ल्ल्ल्ल\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:471]     Ground Truth: परिवार न्यायालय\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:468]     E̲x̲a̲m̲p̲l̲e̲ ̲2̲:\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:469]     Input:        its to see.\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:470]     Prediction:   irericericericericericericericericericeric\n",
      "[NeMo I 2022-06-28 10:46:18 mt_enc_dec_model:471]     Ground Truth: यह देखता रहा है ।\n",
      "[NeMo W 2022-06-28 10:46:18 nemo_logging:349] /opt/conda/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:106: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "      rank_zero_warn(\n",
      "    \n",
      "Epoch 0:   0%|                                         | 0/9271 [00:00<?, ?it/s][W reducer.cpp:1256] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1256] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1256] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1256] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Epoch 0:  53%|███▋   | 4899/9271 [06:27<05:45, 12.64it/s, loss=4.06, v_num=5-45]"
     ]
    }
   ],
   "source": [
    "!HYDRA_FULL_ERROR=1 python enc_dec_nmt.py \\\n",
    "  -cn aayn_base \\\n",
    "  model.preproc_out_dir=preproc \\\n",
    "  model.train_ds.use_tarred_dataset=true \\\n",
    "  model.train_ds.lines_per_dataset_fragment=1000000 \\\n",
    "  model.train_ds.num_batches_per_tarfile=200 \\\n",
    "  model.train_ds.use_tarred_dataset=true \\\n",
    "  model.train_ds.lines_per_dataset_fragment=1000000 \\\n",
    "  model.train_ds.num_batches_per_tarfile=200 \\\n",
    "  model.train_ds.src_file_name=$english_set_tokenized_train \\\n",
    "  model.train_ds.tgt_file_name=$hindi_set_tokenized_train \\\n",
    "  model.train_ds.tokens_in_batch=12500 \\\n",
    "  model.validation_ds.tokens_in_batch=8192 \\\n",
    "  model.validation_ds.src_file_name=$english_set_tokenized_val \\\n",
    "  model.validation_ds.tgt_file_name=$hindi_set_tokenized_val \\\n",
    "  model.encoder_tokenizer.vocab_size=32000 \\\n",
    "  model.decoder_tokenizer.vocab_size=32000 \\\n",
    "  ~model.test_ds \\\n",
    "  model.max_generation_delta=5 \\\n",
    "  model.shared_tokenizer=true \\\n",
    "  model.encoder_tokenizer.tokenizer_model=preproc/tokenizer.BPE.32000.model \\\n",
    "  model.decoder_tokenizer.tokenizer_model=preproc/tokenizer.BPE.32000.model \\\n",
    "  trainer.devices=[0,1,2,3] \\\n",
    "  ~trainer.max_epochs \\\n",
    "  +trainer.max_steps=150000 \\\n",
    "  +exp_manager.exp_dir=results \\\n",
    "  +exp_manager.create_checkpoint_callback=True \\\n",
    "  +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \\\n",
    "  +exp_manager.checkpoint_callback_params.mode=max \\\n",
    "  +exp_manager.checkpoint_callback_params.save_top_k=5 \\\n",
    "  +trainer.val_check_interval=30000 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
