{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training En-Hi NMT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've pre-processed the dataset and can now proceed to train the NMT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining paths\n",
    "\n",
    "Let's define the path to the pre-processed dataset and the directories where the trained models and intermediate artifacts will get saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset root\n",
    "dataset_root = '../dataset/en-hi'\n",
    "%env dataset_root = {dataset_root}\n",
    "\n",
    "# English and Hindi training-validation set\n",
    "%env english_set_tokenized_train = {dataset_root}/final_train_norm_lf_1000_tk_moses.en\n",
    "%env english_set_tokenized_val = {dataset_root}/final_val_norm_lf_1000_tk_moses.en\n",
    "%env hindi_set_tokenized_train = {dataset_root}/final_train_norm_lf_1000_tk_indicnlp.hi\n",
    "%env hindi_set_tokenized_val = {dataset_root}/final_val_norm_lf_1000_tk_indicnlp.hi\n",
    "\n",
    "# Output durectory\n",
    "%env outdir = /preproc/\n",
    "%env results = results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training shared BPE tokenizer and creating tarred dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traditional tokenization methods are particularly problematic when dealing with misspellings and rare words. There have been many suggestions to tackle this problem, and one such strategy is sub-word tokenization.\n",
    "\n",
    "Sub-word tokenization involves breaking down words into subword units that allow the model to make intelligent decisions on words it doesn’t recognize. These subword units can be a character or a string of characters. Byte Pair Encoding (BPE) is a very common subword tokenization technique that relies on counting the most common strings of bytes from data, and then replacing those strings with signifiers from the learned vocabulary. Some other examples of sub-word tokenization techniques are WordPiece and SentencePiece.\n",
    "\n",
    "To facilitate sub-word tokenization, we will train a shared sub-word BPE tokenizer to tokenize both the English and Hindi sentences with a vocabulary size of 32k. NVIDIA NeMo currently supports the [YouTokenToMe](https://github.com/VKCOM/YouTokenToMe) BPE tokenizer.\n",
    "\n",
    "In addition to training the sub-word BPE, we will also create a parallel tarred version of the training dataset. This would help us increase the training throughput by a huge margin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to change the following parameters as per your system confirguration:\n",
    "1. --tokens_in_batch `12500`\n",
    "2. --n_preproc_jobs `2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python create_tarred_parallel_dataset.py --shared_tokenizer --src_fname $english_set_tokenized_train --tgt_fname $hindi_set_tokenized_train --out_dir $outdir --encoder_tokenizer_vocab_size 32000 --decoder_tokenizer_vocab_size 32000 --max_seq_length 512 --tokens_in_batch 12500 --n_preproc_jobs 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving the preproc folder to current directory\n",
    "!mv $outdir ./\n",
    "%env outdir = preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NMT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the prerequisite steps for training the NMT model are complete, we train the model using the Transformer architecture for English to Hindi MT. The encoder and decoder have 6 layers with 8 attention heads per layer with a word embedding size of 512. \n",
    "\n",
    "All the rest of the hyperparameters are a part of the AAYN-base architecture configuration which is passed via a yaml file to the `cn` parameter. The default configuration file for the model can be found [here](https://github.com/NVIDIA/NeMo/blob/main/examples/nlp/machine_translation/conf/aayn_base.yaml). Feel free to experiment with the beam size and length penalty parameters.\n",
    "\n",
    "Let's train the model with the following command.\n",
    "\n",
    "This command will save a `.nemo` file to `exp_manager.exp_dir` that contains the model’s architecture and trained weights. By default, the model is saved as `AAYNBase.nemo`. The argument `trainer.devices` specifies the number of GPUs to use for training. Since we are setting `model.train_ds.use_tarred_dataset=true`, this script will use the parallel tarred dataset that we just created in order to scale the dataloaders during training. We have also specified the path to the trained sub-word BPE tokenizer in `model.encoder_tokenizer.tokenizer_model` and `model.decoder_tokenizer.tokenizer_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HYDRA_FULL_ERROR=1 python enc_dec_nmt.py \\\n",
    "  -cn aayn_base \\\n",
    "  model.preproc_out_dir=$outdir \\\n",
    "  model.train_ds.use_tarred_dataset=true \\\n",
    "  model.train_ds.metadata_file=$outdir/metadata.tokens.12500.json \\\n",
    "  model.train_ds.tokens_in_batch=12500 \\\n",
    "  model.validation_ds.tokens_in_batch=8192 \\\n",
    "  model.validation_ds.src_file_name=$english_set_tokenized_val \\\n",
    "  model.validation_ds.tgt_file_name=$hindi_set_tokenized_val \\\n",
    "  model.encoder_tokenizer.vocab_size=32000 \\\n",
    "  model.decoder_tokenizer.vocab_size=32000 \\\n",
    "  ~model.test_ds \\\n",
    "  model.max_generation_delta=5 \\\n",
    "  model.shared_tokenizer=true \\\n",
    "  model.encoder_tokenizer.tokenizer_model=$outdir/shared_tokenizer.32000.BPE.model \\\n",
    "  model.decoder_tokenizer.tokenizer_model=$outdir/shared_tokenizer.32000.BPE.model \\\n",
    "  trainer.devices=[0,1,2,3] \\\n",
    "  ~trainer.max_epochs \\\n",
    "  +trainer.max_steps=150000 \\\n",
    "  +exp_manager.exp_dir=$results \\\n",
    "  +exp_manager.create_checkpoint_callback=True \\\n",
    "  +exp_manager.checkpoint_callback_params.monitor=val_sacreBLEU \\\n",
    "  +exp_manager.checkpoint_callback_params.mode=max \\\n",
    "  +exp_manager.checkpoint_callback_params.save_top_k=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model will get saved at `$results/AAYNBase/<data-time_of_training>/checkpoints/AAYNBase.nemo`\n",
    "\n",
    "We'll also copy this model to another folder `model` for ease-of-reference during deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../model\n",
    "!cp $results/AAYNBase/<data-time_of_training>/checkpoints/AAYNBase.nemo ../model/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
